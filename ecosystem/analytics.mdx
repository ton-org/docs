---
title: "Analytics and data providers"
sidebarTitle: "Analytics"
---

import { Aside } from '/snippets/aside.jsx';

Developers often need to run analytical queries on top of on-chain data — for example, to track historical changes and aggregate data from multiple accounts.

Since blockchains are not designed for analytical workloads, you need to build an indexing pipeline and run off-chain analytical queries.

Creating such pipelines from scratch can be resource-consuming, so we recommend using one of the tools mentioned on this page.

## Dune analytics

[Dune analytics](https://dune.com/) is one of the leading platforms for running analytical queries and building dashboards. It comes with 100+ blockchain integrations, and TON is among them. Basically, one needs to be familiar with SQL language to write queries, but the [Dune AI](https://docs.dune.com/learning/how-tos/dune-ai-prompt-engineering) prompt engine allows users to start working with data even without SQL knowledge.

### Raw and decoded tables

Dune analytics consumes data from the public [TON Data Lake](#public-data-lake) (see below) and comes with a variety of raw and decoded tables.

The [raw tables](https://dune.com/queries?category=canonical&namespace=ton) include:

- [Blocks](https://docs.dune.com/data-catalog/ton/blocks)
- [Transactions](https://docs.dune.com/data-catalog/ton/transactions)
- [Messages](https://docs.dune.com/data-catalog/ton/messages) — includes raw body and state init data.
- [Balances history](https://docs.dune.com/data-catalog/ton/balances_history) — allows you to get a precise point-in-time balance for any account.
- [Jetton events](https://docs.dune.com/data-catalog/ton/jetton_events) — comes with transfers, burns, and mints.

<Aside>
  Since mints are not covered by the [TEP-74](https://github.com/ton-blockchain/TEPs/blob/master/text/0074-jettons-standard.md) standard, it is not possible to reconstruct balances based solely on jetton events, so the balance history should be used.
</Aside>

Apart from raw tables, there are decoded tables that allow you to work with high-level structures in a unified manner:

- [NFT events](https://dune.com/queries?category=canonical&namespace=ton&id=ton.nft_events) — comprehensive source of NFT-related data, including
sales, transfers, and mints.
- [DEX trades](https://docs.dune.com/data-catalog/ton/dex_trades) — includes a unified data model for DEX trades. The full list of
supported DEXs is available [here](https://github.com/ton-studio/ton-etl/blob/main/datalake/README.md#dex-trades).
- [DEX pools](https://docs.dune.com/data-catalog/ton/dex_pools) — comes with the full history of DEX pool balances and TVL estimations.

Finally, two tables with off-chain metadata are available:

- [Jetton metadata](https://docs.dune.com/data-catalog/ton/jetton_metadata)
- [NFT metadata](https://dune.com/queries?category=canonical&namespace=ton&id=ton.nft_metadata).

### Bespoke data marts

Dune analytics allows projects to build bespoke data marts for each protocol — it is widely used for EVMs with the help of ABIs.

#### Decoding raw data

Since TON handles complex [data structures](/tvm/serialization/cells/) and doesn't have ABIs, a [special decoding framework](https://github.com/duneanalytics/spellbook/blob/main/dbt_subprojects/daily_spellbook/macros/project/ton/README.md) was created. It works on top of the [Spellbook](https://github.com/duneanalytics/spellbook) — a powerful tool for building custom tables with [dbt](https://github.com/dbt-labs/dbt-core) and Jinja macros. It helps decode important information from raw protocol message payloads.

The following protocols are decoded using this framework and serve as examples:

- [EVAA](https://dune.com/queries?category=abstraction&namespace=evaa) ([implementation](https://github.com/duneanalytics/spellbook/tree/main/dbt_subprojects/daily_spellbook/models/evaa/ton))
- [Affluent](https://dune.com/queries?category=abstraction&namespace=affluent) ([implementation](https://github.com/duneanalytics/spellbook/tree/main/dbt_subprojects/daily_spellbook/models/affluent/ton))
- [StormTrade](https://dune.com/queries?category=abstraction&namespace=stormtrade) ([implementation](https://github.com/duneanalytics/spellbook/tree/main/dbt_subprojects/daily_spellbook/models/stormtrade/ton))
- [TON DNS](https://dune.com/queries?category=abstraction&namespace=dns_ton) ([implementation](https://github.com/duneanalytics/spellbook/tree/main/dbt_subprojects/daily_spellbook/models/ton/dns))

#### Custom views

In addition to decoding raw data, the Spellbook allows building custom materialized views. Some of them are widely used and maintained to be up to date:

- [ton.prices_daily](https://dune.com/queries?category=abstraction&namespace=ton&id=ton.prices_daily) — prices calculated based on all other tables. The prices include jettons traded on DEXs, LP tokens for DEXs, perpetuals, tsUSDe, and other core assets. It is recommended to use this table if you need to build an estimation of assets denominated in TON or USD.
- [ton.accounts](https://dune.com/queries?category=abstraction&namespace=ton&id=ton.accounts) — materialized view with information about all accounts. It comes with the latest TON balance, interface (if any), funding information, and other fields.
- [ton.latest_balances](https://dune.com/queries?category=abstraction&namespace=ton&id=ton.latest_balances) — helper table to get the latest balances for TON and Jettons.

<Aside>All tables mentioned above are updated daily.</Aside>

### Getting started with Dune

If you're just starting to explore TON data on Dune, we recommend checking these articles first:

<Columns cols={2}>
  <Card
    title="TON data quick start"
    arrow
    href="https://dune.com/ton_foundation/ton-quick-start"
  />
  <Card
    title="TON on-chain data analysis: quickstart on Dune"
    arrow
    href="https://blog.ton.org/ton-on-chain-data-analysis-dune"
  />
  <Card
    title="How to Analyze TON Users and Token Flows on Dune: A Practical Guide"
    arrow
    href="https://blog.ton.org/how-to-analyze-ton-users-and-token-flows-on-dune"
  />
</Columns>

For inspiration to build your own dashboards, check out these examples:

<Columns cols={2}>
  <Card
    title="Application Activity"
    arrow
    href="https://dune.com/ton_foundation/application-activity"
  />
  <Card
    title="TON & Ethena Boost Rewards Campaign"
    arrow
    href="https://dune.com/ton_foundation/tonandethena-staking-rewards-campaign"
  />
  <Card
    title="Telegram Gifts Dashboard"
    arrow
    href="https://dune.com/rdmcd/telegram-gifts"
  />
</Columns>

## Public Data Lake

Dune integration runs on the public data lake from the [TON-ETL](https://github.com/re-doubt/ton-etl/blob/main/datalake/README.md) project.

[TON-ETL](https://github.com/re-doubt/ton-etl/blob/main/datalake/README.md) is built on top of [TON Center](https://github.com/toncenter) indexer and allows extraction of data from TON Node into data formats suitable for MPP (Massively Parallel Processing) engines: Presto, Apache Spark, etc.

<Aside>
  You can deploy it on your own infrastructure or use publicly available data from the S3 bucket: `s3://aws-public-blockchain/v1.1/ton/`. This dataset is part of the [AWS Public Blockchain Data](https://registry.opendata.aws/aws-public-blockchain/) project and is optimized for use within the AWS big data stack.

  Examples of AWS Athena and AWS Bedrock integration can be found in this [article](https://repost.aws/articles/AR3ABC81yvTPW2ktfHiHPWIA/new-dataset-added-to-the-aws-public-blockchain-data-ton-the-open-network).
</Aside>

The TON-ETL extracts raw data and performs decoding to create a unified view of high-level on-chain activity. The most important part is decoding DEX activity.

The decoding implementation must solve the following tasks:
- Decoding of swap events. The code must check the authenticity of the swap. For example, you cannot rely on the opcode alone since anyone can generate messages with your opcode.
- Extracting all swap-related fields: tokens sold and bought, amounts, query IDs, trader, router (if any), and pool.
- Fetching pool reserves and LP token supply, if applicable.

To add support for a new DEX and decode its activity, you need to prepare a relevant PR on GitHub [to TON-ETL's repo](https://github.com/ton-studio/ton-etl). Use those past PRs as a reference: [BidAsk](https://github.com/ton-studio/ton-etl/pull/186), [CoffeeSwap](https://github.com/ton-studio/ton-etl/pull/171/files), [MemesLab](https://github.com/ton-studio/ton-etl/pull/144).

## Real-time streams

In addition to bulk data export, TON-ETL provides real-time data streaming via Kafka. A [public endpoint](https://github.com/ton-studio/ton-etl/blob/main/datalake/README.md#near-real-time--data-streaming-via-pulic-kafka-topics) is available free of charge for non-profit projects.

For projects that don't meet the non-profit criteria or require an in-house solution, you can deploy the infrastructure yourself by:

1. Running your own [TON node](/ecosystem/node/overview)
2. Launching [ton-etl](https://github.com/re-doubt/ton-etl/blob/main/README.md)
3. Setting up [ton-index-worker](https://github.com/ton-studio/ton-index-worker)

## TON Labels

While data availability and integrations are essential, building insightful dashboards requires enriching data with address labels.

The [TON Labels](https://github.com/ton-studio/ton-labels) project simplifies this process by providing a comprehensive taxonomy of addresses in TON Ecosystem. It covers active addresses across various categories, including centralized exchanges (CEXs), decentralized applications (dApps), and DeFi protocols.

You can access the latest labels either directly from [the build branch](https://github.com/ton-studio/ton-labels/blob/build/assets.json) or through Dune analytics using the [`dune.ton_foundation.dataset_labels`](https://dune.com/queries?category=uploaded_data&id=dune.ton_foundation.dataset_labels) table.

## Other platforms

- [Chainbase](https://docs.chainbase.com/catalog/Ton/Overview) offers a set of raw and decoded tables with TON data. It allows you to run SQL queries and fetch results via API.
- [TON Console](https://docs.tonconsole.com/tonconsole/analytics) provides analysts with Analytics Service.
- [TokenTerminal](https://tokenterminal.com/explorer/projects/the-open-network) comes with high-level metrics across TON Ecosystem.
- [Artemis](https://app.artemisanalytics.com/project/ton?from=projects) contains key metrics for TON and allows you to build customized charts.
